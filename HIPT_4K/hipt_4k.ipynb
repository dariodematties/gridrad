{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738e436-3526-4c1d-a496-3f87a5a26df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from einops import rearrange, repeat\n",
    "from hipt_model_utils import get_vit256, get_vit4k\n",
    "from hipt_heatmap_utils import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1c015-e701-4dfb-9aa4-8ab3d1433c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589322f-926a-49da-bbc4-ac1fe702fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIPT_4K(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    HIPT Model (ViT_4K-256) for encoding non-square images\n",
    "    (with [256 x 256] patch tokens), with [256 x 256] patch\n",
    "    tokens encoded via ViT_256-16 using [16 x 16] patch\n",
    "    tokens\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model256_path = '../HIPT_4K/Checkpoints/vit256_small_dino.pth',\n",
    "                 model4k_path = '../HIPT_4K/Checkpoints/vit4k_xs_dino.pth',\n",
    "                 device256 = torch.device(device),\n",
    "                 device4k = torch.device(device)):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model256 = get_vit256(pretrained_weights=model256_path).to(device)\n",
    "        self.model4k = get_vit4k(pretrained_weights=model4k_path).to(device)\n",
    "        self.device256 = device256\n",
    "        self.device4k = device4k\n",
    "        #self.patch_filter_params = patch_filter_params\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of HIPT (given an image tensor x), outputing the [CLS] token from ViT_4K.\n",
    "        1. x is center-cropped such that the W / H is divisible by the patch token size in ViT_4K.\n",
    "        2. x then gets unfolded into a \"batch\" of [256 x 256] images.\n",
    "        3. A pretrained ViT_256-16 model extracts the CLS token from each [256 x 256] image in the batch.\n",
    "        4. These batch-of-features are then reshaped into a 2D features grid (of width \"w_256\" and height \"h_256\".)\n",
    "        5. This feature grid is then used as the input to ViT_4K-256, outputing [CLS]_4K.\n",
    "\n",
    "        Args:\n",
    "            - x (torch.Tensor): [1 x C x W' x H'] image tensor.\n",
    "        Return:\n",
    "            - features_cls4k (torch.Tensor): [1 x 192] cls token (d_4k = 192 by default).\n",
    "        \"\"\"\n",
    "        # 1. [1 x 3 x W x H].\n",
    "        batch_256, w_256, h_256 = self.prepare_img_tensor(x)\n",
    "        print(f'1. [1 x 3 x W x H] {batch_256.shape}')\n",
    "        # 2. [1 x 3 x w_256 x h_256 x 256 x 256]\n",
    "        batch_256 = batch_256.unfold(2, 256, 256).unfold(3, 256, 256)\n",
    "        print(f'2. [1 x 3 x w_256 x h_256 x 256 x 256] {batch_256.shape}')\n",
    "        # 2. [B x 3 x 256 x 256], where B = (1*w_256*h_256)\n",
    "        batch_256 = rearrange(batch_256, 'b c p1 p2 w h -> (b p1 p2) c w h')\n",
    "        print(f'2. [B x 3 x 256 x 256], where B = (1*w_256*h_256) {batch_256.shape}')\n",
    "\n",
    "        features_cls256 = []\n",
    "        # 3. B may be too large for ViT_256. We further take minibatch\n",
    "        for mini_bs in range(0, batch_256.shape[0], 256):\n",
    "            print(f'Minibatch number {mini_bs}')\n",
    "            minibatch_256 = batch_256[mini_bs:mini_bs+256].to(self.device256, non_blocking=True)\n",
    "            # 3. Extracting ViT_256 features from [256 x 3 x 256 x 256] image batches.\n",
    "            features_cls256.append(self.model256(minibatch_256).detach().cpu())\n",
    "        \n",
    "        print(f'Lenght of the list of minibatches is {len(features_cls256)}')\n",
    "        print(f'Shape of element 0 inside the list {features_cls256[0].shape}')\n",
    "\n",
    "        # 3. [B x 384], where 384 == dim of ViT_256 [CLS] token\n",
    "        features_cls256 = torch.vstack(features_cls256)\n",
    "        print(f'3. [B x 384], where 384 == dim of ViT_256 [CLS] token {features_cls256.shape}')\n",
    "        features_cls256 = features_cls256.reshape(w_256, h_256, 384)\n",
    "        print(features_cls256.shape)\n",
    "        features_cls256 = features_cls256.transpose(0,1).transpose(0,2).unsqueeze(dim=0)\n",
    "        print(features_cls256.shape)\n",
    "        # 4. [1 x 384 x w_256 x h_256]\n",
    "        features_cls256 = features_cls256.to(self.device4k, non_blocking=True)\n",
    "        print(f'4. [1 x 384 x w_256 x h_256] {features_cls256.shape}')\n",
    "        # 5. [1 x 192], where 192 == dim of ViT_4K [CLS] token\n",
    "        features_cls4k = self.model4k.forward(features_cls256)\n",
    "        print(f'5. [1 x 192], where 192 == dim of ViT_4K [CLS] token {features_cls4k.shape}')\n",
    "        \n",
    "        return features_cls4k\n",
    "\n",
    "    def prepare_img_tensor(self, img: torch.Tensor, patch_size=256):\n",
    "        \"\"\"\n",
    "        Helper function that takes a non-square image tensor, and takes a center crop s.t.\n",
    "        the width / height are divisible by 256.\n",
    "\n",
    "        (Note: \"_256\" for w / h should technicaly be renamed as \"_ps\",\n",
    "        but may not be easier to read.\n",
    "        Until I need to make HIPT with patch_sizes != 256,\n",
    "        keeping the naming convention as-is.)\n",
    "\n",
    "        Args:\n",
    "            - img (torch.Tensor): [1 x C x W' x H'] image tensor.\n",
    "            - patch_size (int): Desired patch size to evenly subdivide the image.\n",
    "        Return:\n",
    "            - img_new (torch.Tensor): [1 x C x W x H] image tensor, where W and H\n",
    "            are divisable by patch_size.\n",
    "            - w_256 (int): # of [256 x 256] patches of img_new's width (e.g. - W/256)\n",
    "            - h_256 (int): # of [256 x 256] patches of img_new's height (e.g. - H/256)\n",
    "        \"\"\"\n",
    "        make_divisible = lambda l, patch_size: (l - (l % patch_size))\n",
    "        b, c, w, h = img.shape\n",
    "        #print(b, c, w, h)\n",
    "        load_size = make_divisible(w, patch_size), make_divisible(h, patch_size)\n",
    "        #print(load_size)\n",
    "        w_256, h_256 = w // patch_size, h // patch_size\n",
    "        #print(w_256, h_256)\n",
    "        img_new = transforms.CenterCrop(load_size)(img)\n",
    "        #print(img_new.shape)\n",
    "\n",
    "        return img_new, w_256, h_256\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a6b2c-d84a-4757-a569-d1d43af8174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_transforms():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mean, std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "    eval_t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "    return eval_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bf1c3-f173-480f-ad28-b7dd61f7742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HIPT_4K()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1c8f2-1316-4816-ad74-cf1db1924050",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = Image.open('../HIPT_4K/image_demo/image_4k.png')\n",
    "#region.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690c272-e48d-4973-a887-e0834fb7cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = eval_transforms()(region).unsqueeze(dim=0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9d6ea-601e-4904-89d1-c5e2a3e3b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc7473-8ce2-4432-8f87-8b1fc42d93aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gridrad",
   "language": "python",
   "name": "gridrad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
